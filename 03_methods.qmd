# Methodology

```{r setup, file = "R/chapter_start.R", include = FALSE}
# a number of commands need to run at the beginning of each chapter. This
# includes loading libraries that I always use, as well as options for 
# displaying numbers and text.
```

```{r setup2, include = FALSE}
# Other packages ------
# These are packages that get used in this chapter but aren't part of the default set.

```


This section describes the DBSCAN-TE algorithm and the methodology we develop
to identify its parameters.

## DBSCAN-TE Algorithm
The DBSCAN-TE algorithm is a three-step process to classify activities
from raw location data. 

The first step is a DBSCAN spatial clustering algorithm [@khan2014], which is a classical
density-based algorithm used to find the high-density areas in clustered data.
In this case, DBSCAN is used to identify candidate activity locations as 
places where the data are clustered spatially.
Classical $k$-means algorithms tend to favor symmetrical clusters, and do not
distinguish between points related to an activity and points that are part of a
travel path.
DBSCAN algorithms avoid these potential errors as they require two objective
spatial parameters that both need to be met for a GPS point to be considered
part of a cluster. The first objective spatial parameter is the epsilon
neighborhood distance ($\varepsilon$), or the defined radius that points must
fall within to be considered part of a cluster. The second objective spatial
parameter is the minimum number of points ($\rho$), or the defined minimum
number of points that must be present within that radius to deem it a cluster.
DBSCAN will also identify points that are "noise," or not related to any clusters.


The second step in DBSCAN-TE is the time and entropy step. The classical DBSCAN
algorithm cannot distinguish between clusters that are separated by *time* and not
by space. It is common, for example, for individuals to begin their day at home, 
and return to that same location; DBSCAN would classify both sets of LBS points
as being part of the same activity, because they are in the same spatial location.
A time parameter ($\Delta T$) is used to separate LBS point in the same cluster that are
at least $\Delta T$ units apart. For example, if $\Delta T = 60$ minutes, then 
LBS points in the same cluster that are greater then 60 minutes apart will be
classified into two separate activities. 

Finally, the DBSCAN-TE algorithm considers the "entropy" of points within a
candidate cluster, where entropy is defined as a function of the directions between
consecutive LBS data points. This is important to distinguish clusters 
where the device is constantly (but slowly) moving in same direction --- as might 
happen if the device is stuck in traffic or waiting in a queue --- from clusters
where the person is likely completing a real activity. @fig-entropy illustrates 
the methodology for calculating the entropy.
The declination angle between consecutive points is classified into sectors
of a $2\pi$ circle, and then the entropy is calculated as   [@gong2018]:

$$E = -\sum_{d - 1}^{D}\frac{n_d}{N}\ln\left(\frac{n_d}{N}\right)$$

where $n_d$ is the number of directions falling in sector $d$, $N$ is the total
number of directions in the cluster in question, and $D$ is the total number of
sectors. In our methodology and in @fig-entropy, $D=8$. Low entropy clusters
 will have a high proportion of their
angles falling in the same sector, and high entropy clusters will be more evenly 
distributed around the circle. If the calculated entropy is less than a provided
$\tau$ parameter, then that cluster is disqualified as an activity point.

::: {#fig-entropy layout-row=2 }

![High entropy](img/entropy_high.png){#fig-entropy-high width=290 height=140}
![Low entropy](img/entropy_low.png){#fig-entropy-low width=300 height=140}

Example entropy calculation for two different candidate activity clusters. 
Top: high entropy. Bottom: low entropy.
:::


## Error Measurement and Calibration

We desire to identify the parameters vector $\{\varepsilon, \rho, \Delta T, \tau\}$
that minimizes the "error" between activity points generated by the DBSCAN-TE 
algorithm and a ground-truth collection of labeled activity points for a particular 
user on a particular day. This labeled data is created by manually observing a set
of GPS points and identifying the locations of likely activity points, including 
repeated visits to particular points. 

Let $P_i$ be the set of all location points and time stamps for a user-day $i$.
We can define $L_i$ as the set of points within a particular distance of a labeled
activity point on the user day $i$, and $D_i$ as the set of points within the same
distance of an activity point identified by the DBSCAN-TE algorithm. 
The total error is calculated as
$$E = \sum_{i=1}^N \left[ 1 - \frac{|L_{i}\cap D_{i}| + |P_i\setminus (L_i\cup D_i) | }{|P_i|}\right]$$
where $L_i\cap D_i$ are the points predicted as being part of activities in both
the labeled data and the predicted data, $P_i\setminus (L_i\cup D_i)$ are the 
points that are assigned to an activity cluster in neither dataset, and $|X|$ is 
the traditional set size operator. Thus the error for a individual $i$ is
effectively the number of points for person $i$ that are differently classified
by $L$ and $D$. The total error is the sum of this value for all users, and is the 
objective function we will try to minimize in our calibration. 

To conduct the optimization, we use a simulated annealing algorithm encoded in
the `sannbox` function for R [@pomp]. Simulated annealing is particularly useful
in finding global optima in the presence of large numbers of local optima. The
term “annealing” refers to the cooling of metal in thermodynamics [34]. Hence,
simulated annealing uses the objective function of an optimization problem
instead of the energy of a metal. The algorithm is given an initial set of
parameters and then selects a random move, or random changes in all the given
parameters. If the selected random move improves the solution from the objective
function, error in this case, then it is always accepted. Otherwise, the
algorithm continues the move anyways, but with a probability less than 1. The
worse the move gets, the more the probability exponentially decreases, and the
more likely the move will change toward a direction that improves the
probability.

In addition to the advantageous search design, simulated annealing allows 
us to set box constraints on the parameters, excluding unreasonable values.
The four elements of the parameters vector 
$\{\varepsilon, \rho, \Delta T, \tau\}$ are defined on different scales with 
different units --- the gap between time stamps in separate activities 
$\Delta T$  may be many thousands of seconds while the entropy parameters $\tau$ is
typically a small number between 1 and 5 [@gong2018]. As such, we scale the
parameters in the search algorithm so that the steps made in all parameters are
essentially the same. The starting, lower bound, upper bound, and scale parameters are all given in @tbl-constraints.

```{r tbl-constraints}
#| label: tbl-constraints
#| tbl-cap: Parameter Search Boundaries
tb <- tibble(`Parameter` = c("$\\varepsilon$", "$\\rho$", 
                             "$\\Delta T$", "$\\tau$"),
             `Definition` = c(
               "Radius of activity cluster (in meters)",
               "Minimum number of points to constitute an activity cluster",
               "Time separation (in seconds) between activities",
               "Allowable entropy within an activity cluster"
             ),
             `Starting value` = c(25, 15, 3000, 1.75),
             `Lower bound` = c( 10,   3,       300, 1),
             `Upper bound` = c(100, 300, 12 * 3600, 4),
             `Scale` = c(25, 75, 3600, 1)
) 
library(kableExtra)
kbl(tb, booktabs = TRUE, escape = FALSE) |> 
  kable_styling()
```





## Data 
Data for this research is 

A sample o


