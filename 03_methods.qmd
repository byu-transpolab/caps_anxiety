# Methods

```{r setup, file = "R/chapter_start.R", include = FALSE}
# a number of commands need to run at the beginning of each chapter. This
# includes loading libraries that I always use, as well as options for 
# displaying numbers and text.
```

```{r setup2, include = FALSE}
# Other packages ------
# These are packages that get used in this chapter but aren't part of the default set.

```

## DBSCAN-TE Algorithm
The DBSCAN-TE algorithm classifies activities from raw location data in two steps.
First, a DBSCAN clustering algorithm [@khan2014] finds high-density areas in the
spatial data and excludes noise. This algorithm requires two
parameters: the radius ($\varepsilon$) of an activity cluster; and the minimum
number of cluster points ($\rho$).

The second step is a time and entropy step. DBSCAN alone 
cannot distinguish between clusters that are separated by *time* and not
space. For example, individuals may begin their day at home and return later; 
DBSCAN classifies both sets of points as one activity, because they are at the same location.
A time parameter ($\Delta T$) is used to separate LBS point in the same cluster that are
at least $\Delta T$ units apart. Finally, DBSCAN-TE considers the "entropy" of
points within a candidate cluster, where entropy is a function of the directions
between consecutive LBS data points. This eliminates clusters
where the device is slowly moving in one direction --- as in a traffic queue.
The angle between consecutive points is mapped onto sectors
of a circle, and then the entropy is calculated as :
$$S = -\sum_{d - 1}^{D}\frac{n_d}{N}\ln\left(\frac{n_d}{N}\right)$${#eq-entropy}
where $n_d$ is the number of directions falling in sector $d$, $N$ is the total
number of rays in the cluster, and $D$ is the total number of
sectors (often $8$)[@gong2018]. If $S<\tau$ (a set threshold), then that cluster is
disqualified as an activity point.

## Error Measurement and Calibration

We desire to identify the parameters vector $\{\varepsilon, \rho, \Delta T, \tau\}$
that minimizes the "error" between activity points generated by DBSCAN-TE 
and labeled activity points for a user-day. 
Let $P_i$ be the set of location points for a user-day $i$.
$L_i$ is the set of points within a distance of a labeled
activity point on the user day $i$, and $D_i$ as the set of points within the same
distance of an activity point identified by DBSCAN-TE. 
The total error is
$$ E = \sum_{i=1}^N \left[ 1 - \frac{|L_{i}\cap D_{i}| + |P_i\setminus (L_i\cup D_i) | }{|P_i|}\right]$${#eq-error}
where $L_i\cap D_i$ are the points in both
the labeled data and the predicted data, and $P_i\setminus (L_i\cup D_i)$ are the 
points assigned to an activity cluster in neither.
Thus the error for a user-day $i$ is effectively the share of points $i$ that 
are differently classified by $L$ and $D$. The total error is
the sum of this value for all user days.

To identify parameters minimizing the error functions, we use a simulated 
annealing algorithm in R [@pomp]. Simulated annealing is useful
in finding global optima on non-convex or discontinuous objective functions 
[@bertsimas1993]. Simulated annealing also permits box constraints on the 
parameters and parameter scaling: 
$\{\varepsilon, \rho, \Delta T, \tau\}$ are defined on different scales with 
different units. The constraints and scale parameters are given in
@tbl-constraints.

```{r tbl-constraints}
#| label: tbl-constraints
#| tbl-cap: Parameter Search Boundaries
#| tbl-env: "table*"
tb <- tibble(`Parameter` = c("$\\varepsilon$", "$\\rho$", 
                             "$\\Delta T$", "$\\tau$"),
             `Definition` = c(
               "Radius of cluster [m]",
               "Minimum points to constitute a cluster",
               "Time between activities at same point [s]",
               "Entropy threshold"
             ),
             `Lower bound` = c( 10,   3,       300, 1),
             `Upper bound` = c(100, 300, 12 * 3600, 4),
             `Scale` = c(25, 75, 3600, 1)
) 
library(kableExtra)
kbl(tb, booktabs = TRUE, escape = FALSE) |> 
  kable_styling() 
```

## Data
Data for this study come from a comprehensive longitudinal dataset of 78
volunteers using a mobile application that collects detailed location data. We
drew a random sample of 25 high-quality user-days --- 24 hours of location
points with at least 30 points per minute --- and manually labeled these days
with the points of the visually apparent activities.
