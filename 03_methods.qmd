# Methods

```{r setup, file = "R/chapter_start.R", include = FALSE}
# a number of commands need to run at the beginning of each chapter. This
# includes loading libraries that I always use, as well as options for 
# displaying numbers and text.
```

```{r setup2, include = FALSE}
# Other packages ------
# These are packages that get used in this chapter but aren't part of the default set.

```

## DBSCAN-TE Algorithm
The DBSCAN-TE algorithm is a three-step process to classify activities
from raw location data. 
The first step is a DBSCAN spatial clustering algorithm [@khan2014], which is a classical
density-based algorithm used to find the high-density areas in clustered data and 
exclude points that are labeled as noise. The algorithm requires two parameters. 
The first parameter is the radius ($\varepsilon$) that points fall within to be considered part of a cluster. 
The second parameter is the minimum number of points ($\rho$) in an activity cluster.

The second step in DBSCAN-TE is the time and entropy step. The classical DBSCAN
algorithm cannot distinguish between clusters that are separated by *time* and not
by space. It is common, for example, for individuals to begin their day at home, 
and return to that same location; DBSCAN would classify both sets of points
as being part of the same activity, because they are in the same spatial location.
A time parameter ($\Delta T$) is used to separate LBS point in the same cluster that are
at least $\Delta T$ units apart. For example, if $\Delta T = 60$ minutes, then 
LBS points in the same cluster that are greater then 60 minutes apart will be
classified into two separate activities. 

Finally, the DBSCAN-TE algorithm considers the "entropy" of points within a
candidate cluster, where entropy is defined as a function of the directions between
consecutive LBS data points. This is important to eliminate clusters 
where the device is constantly (but slowly) moving in same direction --- as might 
happen if the device is stuck in traffic or waiting in a queue,
The declination of the ray between consecutive points is classified into sectors
of a circle, and then the entropy is calculated as [@gong2018]:
$$S = -\sum_{d - 1}^{D}\frac{n_d}{N}\ln\left(\frac{n_d}{N}\right)$$
where $n_d$ is the number of directions falling in sector $d$, $N$ is the total
number of rays in the cluster, and $D$ is the total number of
sectors (often $8$). If $S<\tau$ (a set threshold), then that cluster is
disqualified as an activity point.

## Error Measurement and Calibration

We desire to identify the parameters vector $\{\varepsilon, \rho, \Delta T, \tau\}$
that minimizes the "error" between activity points generated by the DBSCAN-TE 
algorithm and a collection of labeled activity points for a user on a day. 
Let $P_i$ be the set of all location points and time stamps for a user-day $i$.
We can define $L_i$ as the set of points within a particular distance of a labeled
activity point on the user day $i$, and $D_i$ as the set of points within the same
distance of an activity point identified by the DBSCAN-TE algorithm. 
The total error is calculated as
$$ E = \sum_{i=1}^N \left[ 1 - \frac{|L_{i}\cap D_{i}| + |P_i\setminus (L_i\cup D_i) | }{|P_i|}\right]$$
where $L_i\cap D_i$ are the points predicted as being part of activities in both
the labeled data and the predicted data, and $P_i\setminus (L_i\cup D_i)$ are the 
points that are assigned to an activity cluster in neither dataset.
Thus the error for a individual $i$ is effectively the share of points for
person $i$ that are differently classified by $L$ and $D$. The total error is
the sum of this value for all users, and is the objective function for the optimization.

To conduct the optimization, we use a simulated annealing algorithm encoded in
the `sannbox` function for R [@pomp]. Simulated annealing is particularly useful
in finding global optima in the presence of large numbers of local optima, or in
objective functions with discontinuities and undefined values [@bertsimas1993]. 
Simulated annealing also permits box constraints on the parameters.
The four elements of the parameters vector
$\{\varepsilon, \rho, \Delta T, \tau\}$ are defined on different scales with 
different units. As such, we scale the parameters in the search algorithm so
that the steps made in all parameters are essentially the same. The starting,
lower bound, upper bound, and scale parameters are all given in
@tbl-constraints.

```{r tbl-constraints}
#| label: tbl-constraints
#| tbl-cap: Parameter Search Boundaries
#| tbl-env: "table*"
tb <- tibble(`Parameter` = c("$\\varepsilon$", "$\\rho$", 
                             "$\\Delta T$", "$\\tau$"),
             `Definition` = c(
               "Radius of cluster [m]",
               "Minimum points to constitute a cluster",
               "Time between activities at same point [s]",
               "Entropy threshold"
             ),
             `Starting value` = c(25, 15, 3000, 1.5),
             `Lower bound` = c( 10,   3,       300, 1),
             `Upper bound` = c(100, 300, 12 * 3600, 4),
             `Scale` = c(25, 75, 3600, 1)
) 
library(kableExtra)
kbl(tb, booktabs = TRUE, escape = FALSE) |> 
  kable_styling(latex_options="scale_down") 
```

## Data
Data for this study come from a comprehensive longitudinal dataset of 78 individuals
undergoing psychotherapeutic treatment using a mobile application who volunteered
to have detailed location data collected by the application. We drew a random sample
of 25 high-quality user-days --- 24 hours of location points with at least 30 points per minute, 
and manually these days with the points of the visually apparent activities. 
